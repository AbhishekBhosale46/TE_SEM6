{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa259e5b-f265-4d9f-b4e9-377616609e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943e7b8-395a-4502-8243-c6d98b4d38db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d64e00-5b23-4c56-9041-c4782300ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/SampleText.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ea2c9-bee4-48e3-93e0-5b8d17933c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f15211-9b81-49c1-9f2c-4816659c96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenization - Tokenization is the process of splitting a text or document into smaller units called tokens. \n",
    "These tokens can be words, phrases, or symbols, depending on the specific tokenizer used. \n",
    "word_tokenize() : split a sentence into tokens or words\n",
    "sent_tokenize() : to split a document or paragraph into sentences\n",
    "\"\"\"\n",
    "word_tokens = word_tokenize(doc)\n",
    "sent_tokens = sent_tokenize(doc)\n",
    "print(word_tokens)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af1661-d89c-4f0e-97c2-a4d8971121f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POS Tagging - POS tagging is the process of assigning a part-of-speech tag (e.g., noun, verb, adjective) \n",
    "to each token in a sentence. It helps in understanding the grammatical structure of a sentence\n",
    "\"\"\"\n",
    "tags = pos_tag(word_tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c056795-083d-4eb3-894d-caef7683baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stop Words Removal - Stop words are common words that are often filtered out from text data because they do not \n",
    "contribute much to the meaning of the text. These words include articles, prepositions, conjunctions, and other common words.\n",
    "\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [token for token in word_tokens if token.lower() not in stop_words]\n",
    "print(len(word_tokens))\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032ffd5-c21e-4ded-8daf-37b3da966943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stemming - Stemming is the process of reducing words to their root or base form by removing affixes (e.g., prefixes, suffixes). \n",
    "The goal of stemming is to reduce words to their common base or root form, which helps in information retrieval and text analysis.\n",
    "Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "\"\"\"\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens = [porter.stem(word) for word in word_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c872cc-347e-4dbe-96c6-30b6d44d0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lemmatization - Lemmatization is similar to stemming but involves reducing words to their base or dictionary form (lemma) \n",
    "using a vocabulary and morphological analysis of the words. Lemmatization ensures that the resulting word is a valid word.\n",
    "Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
    "\"\"\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e151e-d520-408c-aab2-3c214ac12f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Term Frequency - Measures how frequently a term appears in a document. TF measures the frequency of a term (word) in a \n",
    "document relative to the total number of words in that document.\n",
    "TF = (freq of term in a doc / total number of terms in doc)\n",
    "\n",
    "Inverse Document Frequency - IDF measures the rarity of a term across all documents in the corpus.\n",
    "IDF = log(totalno of docs / no of docs containing the term + 1)\n",
    "\n",
    "TF-IDF combines TF and IDF to assign a weight to each term in a document relative to its importance in the entire corpus.\n",
    "\"\"\"\n",
    "\n",
    "def get_tf(docs):\n",
    "    tf = {}\n",
    "    for doc in docs:\n",
    "        tokens = word_tokenize(doc)\n",
    "        total_terms = len(tokens)\n",
    "        for token in set(tokens):\n",
    "            frequency = tokens.count(token)\n",
    "            tf[(token, doc)] = frequency/total_terms\n",
    "    return tf\n",
    "\n",
    "def get_idf(docs):\n",
    "    idf = {}\n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        tokens += word_tokenize(doc)\n",
    "    for token in set(tokens):\n",
    "        count = 1\n",
    "        for d in docs:\n",
    "            if token in word_tokenize(d):\n",
    "                count += 1\n",
    "        idf[token] = math.log(len(docs)/count)\n",
    "    return idf\n",
    "\n",
    "def get_tfidf(docs):\n",
    "    tf = get_tf(docs)\n",
    "    idf = get_idf(docs)\n",
    "    tfidf = {}\n",
    "    for token, doc in tf.keys():\n",
    "        tfidf[(token, doc)] = tf[(token, doc)] * idf[token]\n",
    "    return tfidf\n",
    "\n",
    "doc1 = \"Natural language processing (NLP) is a field of artificial intelligence concerned with the interaction between computers and humans in natural language. It aims to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. NLP techniques are used in a wide range of applications, including machine translation, sentiment analysis, information extraction, and text summarization. One of the key challenges in NLP is dealing with the ambiguity and variability of natural language, which can make it difficult for computers to accurately process and understand text. However, recent advances in machine learning and deep learning have led to significant improvements in NLP performance, making it an increasingly important area of research and development.\"\n",
    "doc2 = \"Machine learning (ML) is a subset of artificial intelligence that focuses on the development of algorithms that can learn from and make predictions or decisions based on data. ML algorithms can be categorized into supervised learning, unsupervised learning, and reinforcement learning, depending on the type of training data and the learning task. Supervised learning involves training a model on labeled data, while unsupervised learning involves training on unlabeled data. Reinforcement learning involves training a model to interact with an environment and learn from feedback. ML techniques have applications in various domains, including image recognition, speech recognition, medical diagnosis, and autonomous vehicles.\"\n",
    "doc3 = \"Data science is an interdisciplinary field that combines techniques from statistics, computer science, and domain-specific knowledge to extract insights and knowledge from data. It involves various stages of the data lifecycle, including data collection, data cleaning, data analysis, and data visualization. Data scientists use a variety of tools and techniques, such as machine learning, statistical modeling, and data mining, to uncover patterns and trends in data and make data-driven decisions. Data science has applications in numerous industries, including healthcare, finance, marketing, and e-commerce.\"\n",
    "\n",
    "tf = get_tf([doc1, doc2, doc3])\n",
    "idf = get_idf([doc1, doc2, doc3])\n",
    "tfidf = get_tfidf([doc1, doc2, doc3])\n",
    "\n",
    "for token, doc in tf.keys():\n",
    "    print(token, \":\", tf[(token, doc)])\n",
    "\n",
    "for token in idf.keys():\n",
    "    print(token, \":\", idf[token])\n",
    "\n",
    "for token, doc in tfidf.keys():\n",
    "    print(token, \":\", tfidf[(token, doc)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
